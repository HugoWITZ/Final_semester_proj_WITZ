{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from src.Cyclic_volt import drug\n",
    "from src.Cyclic_volt import activation, cyclic_voltammogram, plot_cyclic_voltammogram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Dataset (activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs = [\"CP\", \"IFOS\", \"ETOP\", \"FLUR\", \"NAP\", \"DX\"]\n",
    "concentration_drug1 = np.arange(20, 711, 30).tolist()\n",
    "concentration_drug2 = np.arange(20, 711, 30).tolist()\n",
    "\n",
    "scan_rate = [0.0001, 0.001, 0.01]\n",
    "capacitance = [4]\n",
    "num_cycles=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length:  51840\n"
     ]
    }
   ],
   "source": [
    "# Function to save the data in chunks\n",
    "def save_data_chunk(data_chunk, file_name=\"dataset.h5\"):\n",
    "    df_chunk = pd.DataFrame(data_chunk, columns=[\"Drug1\", \"Drug2\", \"Conc1\", \"Conc2\", \"ScanRate\", \"Capacitance\", \"Signal\"])\n",
    "    df_chunk.to_hdf(file_name, key=\"data\", mode=\"a\", append=True, index=False, complevel=9, complib=\"blosc\", min_itemsize={\"Signal\": 140000})\n",
    "\n",
    "def save_potential_chunk(data_chunk, file_name=\"potential.h5\"):\n",
    "    df_chunk = pd.DataFrame(data_chunk, columns=[\"Potential\"])\n",
    "    df_chunk.to_hdf(file_name, key=\"potential\", mode=\"a\", append=True, index=False, complevel=9, complib=\"blosc\")\n",
    "\n",
    "# Create a list to store the data\n",
    "data = []\n",
    "\n",
    "# Counter for iterations\n",
    "counter = 0\n",
    "\n",
    "# Number of iterations after which to save data to CSV\n",
    "save_interval = 1000\n",
    "\n",
    "for drug1 in tqdm(drugs, desc=\"Drugs\", position=0, leave=False):\n",
    "    for drug2 in drugs:\n",
    "        if drug1 != drug2:\n",
    "            for conc1 in concentration_drug1:\n",
    "                for conc2 in concentration_drug2:\n",
    "\n",
    "                    i_drug1_drug2_activ, potential = activation(\n",
    "                        drug1,\n",
    "                        drug2,\n",
    "                        Cs=[conc1, conc2],\n",
    "                        peak_amplitude=[1, 1],\n",
    "                    )\n",
    "\n",
    "                    for scan in scan_rate:\n",
    "                        for cap in capacitance:\n",
    "                            faradaic_current = i_drug1_drug2_activ\n",
    "                            cyclic_potential_data, cyclic_current_data = cyclic_voltammogram(\n",
    "                                potential,\n",
    "                                faradaic_current,\n",
    "                                scan_rate=scan,\n",
    "                                capacitance=cap,\n",
    "                                num_cycles=num_cycles,\n",
    "                            )\n",
    "\n",
    "                            signal_str = \" \".join(map(str, cyclic_current_data))\n",
    "                            data.append([drug1, drug2, conc1, conc2, scan, cap, signal_str])\n",
    "                            \n",
    "                            counter += 1\n",
    "                            \n",
    "                            if counter % save_interval == 0:\n",
    "                                save_data_chunk(data)\n",
    "                                data.clear()\n",
    "\n",
    "# Save the remaining data\n",
    "if data:\n",
    "    save_data_chunk(data)\n",
    "\n",
    "# Save the potential data to use for plotting later\n",
    "potential_str = \" \".join(map(str, cyclic_potential_data))\n",
    "save_potential_chunk([potential_str])\n",
    "\n",
    "print(\"Data length: \", counter)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset in dataset.h5: 51840\n"
     ]
    }
   ],
   "source": [
    "chunksize = 1000  # Adjust the chunk size as needed\n",
    "input_file = \"dataset.h5\"\n",
    "output_file = \"processed_data.h5\"\n",
    "key = \"data\"\n",
    "\n",
    "with pd.HDFStore(input_file, mode=\"r\") as hdf:\n",
    "    num_rows = hdf.get_storer(key).nrows\n",
    "\n",
    "print(f\"Length of the dataset in {input_file}: {num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(chunk, drug_dict):\n",
    "    chunk[\"Sensitivity1\"] = chunk[\"Drug1\"].apply(lambda x: drug_dict[x][\"Sensitivity\"]).astype(\"float64\")\n",
    "    chunk[\"Sensitivity2\"] = chunk[\"Drug2\"].apply(lambda x: drug_dict[x][\"Sensitivity\"]).astype(\"float64\")\n",
    "    chunk[\"Peak_pos1\"] = chunk[\"Drug1\"].apply(lambda x: drug_dict[x][\"Peak_pos\"]).astype(\"float64\")\n",
    "    chunk[\"Peak_pos2\"] = chunk[\"Drug2\"].apply(lambda x: drug_dict[x][\"Peak_pos\"]).astype(\"float64\")\n",
    "    chunk[\"Peak_width1\"] = chunk[\"Drug1\"].apply(lambda x: drug_dict[x][\"Peak_width\"]).astype(\"float64\")\n",
    "    chunk[\"Peak_width2\"] = chunk[\"Drug2\"].apply(lambda x: drug_dict[x][\"Peak_width\"]).astype(\"float64\")\n",
    "    chunk[\"k_m1\"] = chunk[\"Drug1\"].apply(lambda x: drug_dict[x][\"k_m\"]).astype(\"float64\")\n",
    "    chunk[\"k_m2\"] = chunk[\"Drug2\"].apply(lambda x: drug_dict[x][\"k_m\"]).astype(\"float64\")\n",
    "    chunk[\"v_max1\"] = chunk[\"Drug1\"].apply(lambda x: drug_dict[x][\"v_max\"]).astype(\"float64\")\n",
    "    chunk[\"v_max2\"] = chunk[\"Drug2\"].apply(lambda x: drug_dict[x][\"v_max\"]).astype(\"float64\")\n",
    "    return chunk\n",
    "\n",
    "def normalize_chunk(chunk, feature, max_feature):\n",
    "    # Normalize the Signal column using the global maximum value\n",
    "    chunk[feature] = chunk[feature].apply(lambda x: x / max_feature)\n",
    "    # Add other tasks as needed\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature names to be normalized\n",
    "feature_names = [\n",
    "    \"Conc1\", \"Conc2\", \"ScanRate\", \"Capacitance\",\n",
    "    \"Sensitivity1\", \"Sensitivity2\",\n",
    "    \"Peak_pos1\", \"Peak_pos2\",\n",
    "    \"Peak_width1\", \"Peak_width2\",\n",
    "    \"k_m1\", \"k_m2\",\n",
    "    \"v_max1\", \"v_max2\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add features to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the list of start indexes for each chunk\n",
    "chunk_start = [i for i in range(0, num_rows, chunksize)]\n",
    "chunk_start.append(num_rows)  # Append the last element (the remaining rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 52/52 [01:16<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "########### ADD FEATURE ###########\n",
    "\n",
    "for i in tqdm(range(len(chunk_start) - 1), desc=\"Processing chunks\"):\n",
    "\n",
    "    # Add features to the chunk\n",
    "    chunk_df = pd.read_hdf(\n",
    "        \"dataset.h5\", key=\"data\", start=chunk_start[i], stop=chunk_start[i + 1]\n",
    "    )\n",
    "    chunk_with_features = add_features(chunk_df, drug)\n",
    "\n",
    "    # Save the processed chunk to the output HDF5 file\n",
    "    if i == 0:\n",
    "        # If it's the first chunk, create a new HDF5 file\n",
    "        chunk_with_features.to_hdf(\n",
    "            output_file,\n",
    "            key=\"data\",\n",
    "            mode=\"w\",\n",
    "            index=False,\n",
    "            complevel=9,\n",
    "            complib=\"blosc\",\n",
    "            format=\"table\",\n",
    "            min_itemsize={\"Signal\": 140000, \"Drug1\": 20, \"Drug2\": 20},\n",
    "            #data_columns=data_columns,\n",
    "        )\n",
    "    else:\n",
    "        # If it's not the first chunk, append to the existing HDF5 file\n",
    "        chunk_with_features.to_hdf(\n",
    "            output_file,\n",
    "            key=\"data\",\n",
    "            mode=\"a\",\n",
    "            append=True,\n",
    "            index=False,\n",
    "            complevel=9,\n",
    "            complib=\"blosc\",\n",
    "            format=\"table\",\n",
    "            min_itemsize={\"Signal\": 140000, \"Drug1\": 20, \"Drug2\": 20},\n",
    "            #data_columns=data_columns,\n",
    "        )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for maximum and normalize by chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching max in chunks: 100%|██████████| 52/52 [00:19<00:00,  2.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the global maximum values for each feature\n",
    "max_dict = {feature: -float(\"inf\") for feature in feature_names}\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(chunk_start) - 1), desc=\"Searching max in chunks\"):\n",
    "    # Read the chunk\n",
    "    chunk_df = pd.read_hdf(\n",
    "        \"processed_data.h5\", key=\"data\", start=chunk_start[i], stop=chunk_start[i + 1]\n",
    "    )\n",
    "\n",
    "    # Update the global maximum values for each feature\n",
    "    for feature in max_dict:\n",
    "        if feature in chunk_df.columns:\n",
    "            max_value = chunk_df[feature].apply(abs).max()\n",
    "            if max_value > max_dict[feature]:\n",
    "                max_dict[feature] = max_value\n",
    "        else:\n",
    "            print(f\"Column '{feature}' not found in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing and normalizing chunks: 100%|██████████| 52/52 [01:19<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "# APPLIED NORMALIZATION TO ALL CHUNKS\n",
    "####################################\n",
    "normalized = \"data_normalized.h5\"\n",
    "# Process chunks and write to a new output file\n",
    "for i in tqdm(range(len(chunk_start) - 1), desc=\"Processing and normalizing chunks\"):\n",
    "    # Read the chunk\n",
    "    chunk_df = pd.read_hdf(\n",
    "        \"processed_data.h5\", key=\"data\", start=chunk_start[i], stop=chunk_start[i + 1]\n",
    "    )\n",
    "\n",
    "    # Normalize the chunk\n",
    "    for feature, max_value in max_dict.items():\n",
    "        if feature in chunk_df.columns:\n",
    "            chunk_normalized = normalize_chunk(chunk_df, feature, max_value)\n",
    "        else:\n",
    "            print(f\"Column '{feature}' not found in the DataFrame.\")\n",
    "\n",
    "    chunk_df[[\"Drug1\", \"Drug2\"]] = chunk_df[[\"Drug1\", \"Drug2\"]].replace({\"CP\": 0, \"IFOS\": 1, \"ETOP\": 2, \"FLUR\": 3, \"NAP\": 4, \"DX\": 5})\n",
    "\n",
    "    # Save the processed chunk to the output HDF5 file\n",
    "    if i == 0:\n",
    "        # If it's the first chunk, create a new HDF5 file\n",
    "        chunk_normalized.to_hdf(\n",
    "            normalized,\n",
    "            key=\"data\",\n",
    "            mode=\"w\",\n",
    "            index=False,\n",
    "            complevel=9,\n",
    "            complib=\"blosc\",\n",
    "            format=\"table\",\n",
    "            min_itemsize={\"Signal\": 140000, \"Drug1\": 20, \"Drug2\": 20}\n",
    "        )\n",
    "    else:\n",
    "        # If it's not the first chunk, append to the existing HDF5 file\n",
    "        chunk_normalized.to_hdf(\n",
    "            normalized,\n",
    "            key=\"data\",\n",
    "            mode=\"a\",\n",
    "            append=True,\n",
    "            index=False,\n",
    "            complevel=9,\n",
    "            complib=\"blosc\",\n",
    "            format=\"table\",\n",
    "            min_itemsize={\"Signal\": 140000, \"Drug1\": 20, \"Drug2\": 20}\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the full dataset and convert it into .feather file to speed up the loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear variables\n",
    "del signal_str\n",
    "del potential_str\n",
    "\n",
    "# delete processed data file\n",
    "os.remove(\"processed_data.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the normalized data\n",
    "df = pd.read_hdf(\"data_normalized.h5\", key=\"data\")\n",
    "df.head()\n",
    "\n",
    "#reset index\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as feather\n",
    "df.to_feather(\"normalized_data.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drug1</th>\n",
       "      <th>Drug2</th>\n",
       "      <th>Conc1</th>\n",
       "      <th>Conc2</th>\n",
       "      <th>ScanRate</th>\n",
       "      <th>Capacitance</th>\n",
       "      <th>Signal</th>\n",
       "      <th>Sensitivity1</th>\n",
       "      <th>Sensitivity2</th>\n",
       "      <th>Peak_pos1</th>\n",
       "      <th>Peak_pos2</th>\n",
       "      <th>Peak_width1</th>\n",
       "      <th>Peak_width2</th>\n",
       "      <th>k_m1</th>\n",
       "      <th>k_m2</th>\n",
       "      <th>v_max1</th>\n",
       "      <th>v_max2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.006850648514866037 33.39623902311649 66.7856...</td>\n",
       "      <td>0.069231</td>\n",
       "      <td>0.043956</td>\n",
       "      <td>-0.657778</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.006706</td>\n",
       "      <td>0.003499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.006850648514866037 3.3461764536005996 6.6855...</td>\n",
       "      <td>0.069231</td>\n",
       "      <td>0.043956</td>\n",
       "      <td>-0.657778</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.006706</td>\n",
       "      <td>0.003499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.006850648514866037 0.34117019664901005 0.675...</td>\n",
       "      <td>0.069231</td>\n",
       "      <td>0.043956</td>\n",
       "      <td>-0.657778</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.006706</td>\n",
       "      <td>0.003499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.070423</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.006858229126972536 33.396248401363984 66.785...</td>\n",
       "      <td>0.069231</td>\n",
       "      <td>0.043956</td>\n",
       "      <td>-0.657778</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.006706</td>\n",
       "      <td>0.003499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.070423</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.006858229126972536 3.3461858318480897 6.6855...</td>\n",
       "      <td>0.069231</td>\n",
       "      <td>0.043956</td>\n",
       "      <td>-0.657778</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.006706</td>\n",
       "      <td>0.003499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Drug1  Drug2     Conc1     Conc2  ScanRate  Capacitance   \n",
       "0      0      1  0.028169  0.028169      0.01          1.0  \\\n",
       "1      0      1  0.028169  0.028169      0.10          1.0   \n",
       "2      0      1  0.028169  0.028169      1.00          1.0   \n",
       "3      0      1  0.028169  0.070423      0.01          1.0   \n",
       "4      0      1  0.028169  0.070423      0.10          1.0   \n",
       "\n",
       "                                              Signal  Sensitivity1   \n",
       "0  0.006850648514866037 33.39623902311649 66.7856...      0.069231  \\\n",
       "1  0.006850648514866037 3.3461764536005996 6.6855...      0.069231   \n",
       "2  0.006850648514866037 0.34117019664901005 0.675...      0.069231   \n",
       "3  0.006858229126972536 33.396248401363984 66.785...      0.069231   \n",
       "4  0.006858229126972536 3.3461858318480897 6.6855...      0.069231   \n",
       "\n",
       "   Sensitivity2  Peak_pos1  Peak_pos2  Peak_width1  Peak_width2  k_m1   \n",
       "0      0.043956  -0.657778       -1.0          1.0        0.375   1.0  \\\n",
       "1      0.043956  -0.657778       -1.0          1.0        0.375   1.0   \n",
       "2      0.043956  -0.657778       -1.0          1.0        0.375   1.0   \n",
       "3      0.043956  -0.657778       -1.0          1.0        0.375   1.0   \n",
       "4      0.043956  -0.657778       -1.0          1.0        0.375   1.0   \n",
       "\n",
       "       k_m2    v_max1    v_max2  \n",
       "0  0.001157  0.006706  0.003499  \n",
       "1  0.001157  0.006706  0.003499  \n",
       "2  0.001157  0.006706  0.003499  \n",
       "3  0.001157  0.006706  0.003499  \n",
       "4  0.001157  0.006706  0.003499  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the normalized data\n",
    "df = pd.read_feather(\"normalized_data.feather\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRUGS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
